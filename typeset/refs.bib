@article{Martens2011,
    author = {Martens, James},
    doi = {2},
    file = {:home/fliang/Documents/reading/rnn-lstm/ICML2011Sutskever{\_}524.pdf:pdf},
    isbn = {9781450306195},
    issn = {1},
    journal = {Neural Networks},
    number = {1},
    pages = {1017--1024},
    title = {{Generating Text with Recurrent Neural Networks}},
    url = {http://www.icml-2011.org/papers/524{\_}icmlpaper.pdf},
    volume = {131},
    year = {2011}
}
@article{Mikolov2011,
    author = {Mikolov, Toma and Kombrink, Stefan and Burget, Luka and Cernocky, Jan and Khudanpur, Sanjeev},
    doi = {10.1109/ICASSP.2011.5947611},
    file = {:home/fliang/Documents/reading/rnn-lstm/mikolov{\_}icassp2011{\_}5528.pdf:pdf},
    isbn = {9781457705397},
    issn = {15206149},
    journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
    keywords = {Backpropagation algorithms,Computational complexity,Computational linguistics,Signal processing,Speech communication,Speech recognition},
    pages = {5528--5531},
    title = {{Extensions of recurrent neural network language model}},
    url = {http://dx.doi.org/10.1109/ICASSP.2011.5947611},
    year = {2011}
}
@article{Mikolov2010,
    author = {Mikolov, T and Karafiat, M and Burget, L and Cernocky, J and Khudanpur, S},
    file = {:home/fliang/Documents/reading/rnn-lstm/mikolov{\_}interspeech2010{\_}IS100722.pdf:pdf},
    journal = {Interspeech},
    number = {September},
    pages = {1045--1048},
    title = {{Recurrent Neural Network based Language Model}},
    year = {2010}
}
@article{Graves2005,
    abstract = {In this paper, we present bidirectional Long Short Term Memory (LSTM) networks, and a modified, full gradient version of the LSTM learning algorithm. We evaluate Bidirectional LSTM (BLSTM) and several other network architectures on the benchmark task of framewise phoneme classification, using the TIMIT database. Our main findings are that bidirectional networks outperform unidirectional ones, and Long Short Term Memory (LSTM) is much faster and also more accurate than both standard Recurrent Neural Nets (RNNs) and time-windowed Multilayer Perceptrons (MLPs). Our results support the view that contextual information is crucial to speech processing, and suggest that BLSTM is an effective architecture with which to exploit it. ?? 2005 Elsevier Ltd. All rights reserved.},
    author = {Graves, Alex and Schmidhuber, J??rgen},
    doi = {10.1109/IJCNN.2005.1556215},
    file = {:home/fliang/Documents/eBooks/AI-ML/deep-learning-reading-list-spring14/deep-learning-reading-list/nlp-speech/ijcnn2005.pdf:pdf},
    isbn = {0780390482},
    issn = {08936080},
    journal = {Proceedings of the International Joint Conference on Neural Networks},
    mendeley-groups = {BachBot},
    pages = {2047--2052},
    pmid = {16112549},
    title = {{Framewise phoneme classification with bidirectional LSTM networks}},
    volume = {4},
    year = {2005}
}
@article{Bahdanau2015,
    archivePrefix = {arXiv},
    arxivId = {1409.0473},
    author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
    eprint = {1409.0473},
    file = {:home/fliang/Documents/eBooks/AI-ML/shane{\_}LSTM/1409.0473v6.pdf:pdf},
    mendeley-groups = {BachBot},
    pages = {1--15},
    title = {{{\#}4 Neural Machine Translation by Jointly Learning to Align and Translate}},
    url = {http://arxiv.org/pdf/1409.0473v6.pdf},
    year = {2015}
}
@article{Liu2014,
    abstract = {In this paper, we propose a novel recursive recurrent neural network (R2NN) to model the end-to-end decoding process for statistical machine translation. R2NN is a combination of recursive neural network and recurrent neural network, and in turn integrates their respective capabilities: (1) new information can be used to generate the next hidden state, like recurrent neural networks, so that language model and translation model can be integrated naturally; (2) a tree structure can be built, as recursive neural networks, so as to generate the translation candidates in a bottom up manner. A semi-supervised training approach is proposed to train the parameters, and the phrase pair embedding is explored to model translation confidence directly. Experiments on a Chinese to English translation task show that our proposed R2NN can outperform the state-of-the-art baseline by about 1.5 points in BLEU.},
    author = {Liu, Shujie and Yang, Nan and Li, Mu and Zhou, Ming},
    file = {:home/fliang/Documents/reading/rnn-lstm/P14-1140.pdf:pdf},
    journal = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014)},
    mendeley-groups = {BachBot},
    pages = {1491--1500},
    title = {{A Recursive Recurrent Neural Network for Statistical Machine Translation}},
    year = {2014}
}
@article{Auli2013,
    abstract = {We present a joint language and transla-tion model based on a recurrent neural net-work which predicts target words based on an unbounded history of both source and tar-get words. The weaker independence as-sumptions of this model result in a vastly larger search space compared to related feed-forward-based language or translation models. We tackle this issue with a new lattice rescor-ing algorithm and demonstrate its effective-ness empirically. Our joint model builds on a well known recurrent neural network language model (Mikolov, 2012) augmented by a layer of additional inputs from the source language. We show competitive accuracy compared to the traditional channel model features. Our best results improve the output of a system trained on WMT 2012 French-English data by up to 1.5 BLEU, and by 1.1 BLEU on average across several test sets.},
    author = {Auli, Michael and Galley, Michel and Quirk, Chris and Zweig, Geoffrey},
    file = {:home/fliang/Documents/reading/rnn-lstm/EMNLP2013RNNMT.pdf:pdf},
    isbn = {9781937284978},
    journal = {Emnlp},
    mendeley-groups = {BachBot},
    number = {October},
    pages = {1044--1054},
    title = {{Joint Language and Translation Modeling with Recurrent Neural Networks.}},
    url = {http://research.microsoft.com/en-us/um/people/gzweig/Pubs/EMNLP2013RNNMT.pdf},
    year = {2013}
}
@article{Sutskever2014,
    abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excel-lent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Fi-nally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
    archivePrefix = {arXiv},
    arxivId = {1409.3215},
    author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
    eprint = {1409.3215},
    file = {:home/fliang/Documents/reading/rnn-lstm/5346-sequence-to-sequence-learning-with-neural-networks.pdf:pdf},
    isbn = {1409.3215},
    journal = {Advances in Neural Information Processing Systems (NIPS)},
    mendeley-groups = {BachBot},
    pages = {3104--3112},
    title = {{Sequence to sequence learning with neural networks}},
    url = {http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural},
    year = {2014}
}
@article{Collobert2011,
    abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to var- ious natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational re- quirements.},
    archivePrefix = {arXiv},
    arxivId = {1103.0398},
    author = {Collobert, R. and Weston, J. and Bottou, L. and Karlen, M. and Kavukcuoglu, K. and Kuksa, P.},
    doi = {10.1145/2347736.2347755},
    eprint = {1103.0398},
    file = {:home/fliang/Documents/15-16/cambridge/research/papers/1103.0398v1.pdf:pdf},
    isbn = {1532-4435},
    issn = {1532-4435},
    journal = {Journal of Machine Learning Research},
    keywords = {and semantic role labeling,be applied,by trying,chunking,learning algorithm that can,named entity recognition,natural language processing,neural network architecture and,neural networks,part-of-speech tagging,processing tasks including,this versatility is achieved,to various natural language,we propose a unified},
    pages = {2493--2537},
    pmid = {1000183096},
    title = {{Natural Language Processing (Almost) from Scratch}},
    volume = {12},
    year = {2011}
}

