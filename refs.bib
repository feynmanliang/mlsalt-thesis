@article{Martens2011,
    author = {Martens, James},
    doi = {2},
    file = {:home/fliang/Documents/reading/rnn-lstm/ICML2011Sutskever{\_}524.pdf:pdf},
    isbn = {9781450306195},
    issn = {1},
    journal = {Neural Networks},
    number = {1},
    pages = {1017--1024},
    title = {{Generating Text with Recurrent Neural Networks}},
    url = {http://www.icml-2011.org/papers/524{\_}icmlpaper.pdf},
    volume = {131},
    year = {2011}
}
@article{Mikolov2011,
    author = {Mikolov, Toma and Kombrink, Stefan and Burget, Luka and Cernocky, Jan and Khudanpur, Sanjeev},
    doi = {10.1109/ICASSP.2011.5947611},
    file = {:home/fliang/Documents/reading/rnn-lstm/mikolov{\_}icassp2011{\_}5528.pdf:pdf},
    isbn = {9781457705397},
    issn = {15206149},
    journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
    keywords = {Backpropagation algorithms,Computational complexity,Computational linguistics,Signal processing,Speech communication,Speech recognition},
    pages = {5528--5531},
    title = {{Extensions of recurrent neural network language model}},
    url = {http://dx.doi.org/10.1109/ICASSP.2011.5947611},
    year = {2011}
}
@article{Mikolov2010,
    author = {Mikolov, T and Karafiat, M and Burget, L and Cernocky, J and Khudanpur, S},
    file = {:home/fliang/Documents/reading/rnn-lstm/mikolov{\_}interspeech2010{\_}IS100722.pdf:pdf},
    journal = {Interspeech},
    number = {September},
    pages = {1045--1048},
    title = {{Recurrent Neural Network based Language Model}},
    year = {2010}
}
@article{Graves2005,
    abstract = {In this paper, we present bidirectional Long Short Term Memory (LSTM) networks, and a modified, full gradient version of the LSTM learning algorithm. We evaluate Bidirectional LSTM (BLSTM) and several other network architectures on the benchmark task of framewise phoneme classification, using the TIMIT database. Our main findings are that bidirectional networks outperform unidirectional ones, and Long Short Term Memory (LSTM) is much faster and also more accurate than both standard Recurrent Neural Nets (RNNs) and time-windowed Multilayer Perceptrons (MLPs). Our results support the view that contextual information is crucial to speech processing, and suggest that BLSTM is an effective architecture with which to exploit it. ?? 2005 Elsevier Ltd. All rights reserved.},
    author = {Graves, Alex and Schmidhuber, J??rgen},
    doi = {10.1109/IJCNN.2005.1556215},
    file = {:home/fliang/Documents/eBooks/AI-ML/deep-learning-reading-list-spring14/deep-learning-reading-list/nlp-speech/ijcnn2005.pdf:pdf},
    isbn = {0780390482},
    issn = {08936080},
    journal = {Proceedings of the International Joint Conference on Neural Networks},
    mendeley-groups = {BachBot},
    pages = {2047--2052},
    pmid = {16112549},
    title = {{Framewise phoneme classification with bidirectional LSTM networks}},
    volume = {4},
    year = {2005}
}
@article{Bahdanau2015,
    archivePrefix = {arXiv},
    arxivId = {1409.0473},
    author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
    eprint = {1409.0473},
    file = {:home/fliang/Documents/eBooks/AI-ML/shane{\_}LSTM/1409.0473v6.pdf:pdf},
    mendeley-groups = {BachBot},
    pages = {1--15},
    title = {{{\#}4 Neural Machine Translation by Jointly Learning to Align and Translate}},
    url = {http://arxiv.org/pdf/1409.0473v6.pdf},
    year = {2015}
}
@article{Auli2013,
    abstract = {We present a joint language and transla-tion model based on a recurrent neural net-work which predicts target words based on an unbounded history of both source and tar-get words. The weaker independence as-sumptions of this model result in a vastly larger search space compared to related feed-forward-based language or translation models. We tackle this issue with a new lattice rescor-ing algorithm and demonstrate its effective-ness empirically. Our joint model builds on a well known recurrent neural network language model (Mikolov, 2012) augmented by a layer of additional inputs from the source language. We show competitive accuracy compared to the traditional channel model features. Our best results improve the output of a system trained on WMT 2012 French-English data by up to 1.5 BLEU, and by 1.1 BLEU on average across several test sets.},
    author = {Auli, Michael and Galley, Michel and Quirk, Chris and Zweig, Geoffrey},
    file = {:home/fliang/Documents/reading/rnn-lstm/EMNLP2013RNNMT.pdf:pdf},
    isbn = {9781937284978},
    journal = {Emnlp},
    mendeley-groups = {BachBot},
    number = {October},
    pages = {1044--1054},
    title = {{Joint Language and Translation Modeling with Recurrent Neural Networks.}},
    url = {http://research.microsoft.com/en-us/um/people/gzweig/Pubs/EMNLP2013RNNMT.pdf},
    year = {2013}
}
@article{Sutskever2014,
    abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excel-lent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Fi-nally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
    archivePrefix = {arXiv},
    arxivId = {1409.3215},
    author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
    eprint = {1409.3215},
    file = {:home/fliang/Documents/reading/rnn-lstm/5346-sequence-to-sequence-learning-with-neural-networks.pdf:pdf},
    isbn = {1409.3215},
    journal = {Advances in Neural Information Processing Systems (NIPS)},
    mendeley-groups = {BachBot},
    pages = {3104--3112},
    title = {{Sequence to sequence learning with neural networks}},
    url = {http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural},
    year = {2014}
}
@article{Collobert2011,
    abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to var- ious natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational re- quirements.},
    archivePrefix = {arXiv},
    arxivId = {1103.0398},
    author = {Collobert, R. and Weston, J. and Bottou, L. and Karlen, M. and Kavukcuoglu, K. and Kuksa, P.},
    doi = {10.1145/2347736.2347755},
    eprint = {1103.0398},
    file = {:home/fliang/Documents/15-16/cambridge/research/papers/1103.0398v1.pdf:pdf},
    isbn = {1532-4435},
    issn = {1532-4435},
    journal = {Journal of Machine Learning Research},
    keywords = {and semantic role labeling,be applied,by trying,chunking,learning algorithm that can,named entity recognition,natural language processing,neural network architecture and,neural networks,part-of-speech tagging,processing tasks including,this versatility is achieved,to various natural language,we propose a unified},
    pages = {2493--2537},
    pmid = {1000183096},
    title = {{Natural Language Processing (Almost) from Scratch}},
    volume = {12},
    year = {2011}
}
@article{Brien2016,
    author = {Brien, Tim O and Roman, Iran},
    file = {:home/fliang/Documents/15-16/cambridge/research/papers/final.pdf:pdf},
    journal = {CS224d: Deep Learning for Natural Language Processing Final Projects},
    mendeley-groups = {Thesis/Related work},
    pages = {1--9},
    title = {{A Recurrent Neural Network for Musical Structure Processing and Expectation}},
    year = {2016}
}
@article{Allan2005,
    author = {Allan, Moray and Williams, Christopher KI},
    file = {:home/fliang/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - allan2005.pdf:pdf},
    journal = {Advances in Neural Information Processing Systems},
    mendeley-groups = {Thesis,Thesis/Harmonization},
    pages = {25--32},
    title = {allan2005},
    volume = {17},
    year = {2005}
}
@article{Boulanger-Lewandowski2012,
    abstract = {We investigate the problem of modeling symbolic sequences of polyphonic music in a completely general piano-roll representation. We introduce a probabilistic model based on distribution estimators conditioned on a recurrent neural network that is able to discover temporal dependencies in high-dimensional sequences. Our approach outperforms many traditional models of polyphonic music on a variety of realistic datasets. We show how our musical language model can serve as a symbolic prior to improve the accuracy of polyphonic transcription.},
    archivePrefix = {arXiv},
    arxivId = {1206.6392},
    author = {Boulanger-Lewandowski, Nicolas and Vincent, Pascal and Bengio, Yoshua},
    eprint = {1206.6392},
    file = {:home/fliang/Documents/reading/rnn-lstm/1206.6392v1.pdf:pdf},
    isbn = {978-1-4503-1285-1},
    journal = {Proceedings of the 29th International Conference on Machine Learning (ICML-12)},
    mendeley-groups = {Thesis/Related work},
    number = {Cd},
    pages = {1159--1166},
    title = {{Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription}},
    year = {2012}
}
@article{Lyu2015,
    author = {Lyu, Qi},
    file = {:home/fliang/Documents/15-16/cambridge/research/papers/p991-lyu.pdf:pdf},
    isbn = {9781450334594},
    journal = {Proceedings of the 23rd Annual ACM Conference on Multimedia Conference},
    pages = {991--994},
    title = {{Polyphonic Music Modelling with LSTM-RTRBM}},
    year = {2015}
}
@article{Eck2002,
    abstract = {In general music composed by recurrent neural networks (RNNs) suffers from a lack of global structure. Though networks can learn note-by-note transition probabilities and even reproduce phrases, attempts at learning an entire musical form and using that knowledge to guide composition have been unsuccessful. The reason for this failure seems to be that RNNs cannot keep track of temporally distant events that indicate global music structure. Long Short-Term Memory (LSTM) has succeeded in similar domains where other RNNs have failed, such as timing {\&} counting and CSL learning. In the current study we show that LSTM is also a good mechanism for learning to compose music. We compare this approach to previous attempts, with particular focus on issues of data representation. We present experimental results showing that LSTM successfully learns a form of blues music and is able to compose novel (and we believe pleasing) melodies in that style. Remarkably, once the network has found the relevant structure it does not drift from it: LSTM is able to play the blues with good timing and proper structure as long as one is willing to listen.},
    author = {Eck, Douglas and Schmidhuber, J{\"{u}}rgen},
    file = {:home/fliang/Documents/15-16/cambridge/research/papers/IDSIA-07-02.pdf:pdf},
    journal = {Idsia},
    title = {{A First Look at Music Composition using LSTM Recurrent Neural Networks}},
    url = {http://www.idsia.ch/{~}juergen/blues/IDSIA-07-02.pdf},
    year = {2002}
}
@article{Eck2002-blues,
    abstract = {Few types of signal streams are as ubiquitous as music. Here we consider the problem of extracting essential ingredients of music signals, such as well-defined global temporal structure in the form of nested periodicities (or meter). Can we construct an adaptive signal processing device that learns by example how to generate new instances of a given musical style? Because recurrent neural networks can in principle learn the temporal structure of a signal, they are good candidates for such a task. Unfortunately, music composed by standard recurrent neural networks (RNNs) often lacks global coherence. The reason for this failure seems to be that RNNs cannot keep track of temporally distant events that indicate global music structure. Long Short-Term Memory (LSTM) has succeeded in similar domains where other RNNs have failed, such as timing {\&} counting and learning of context sensitive languages. In the current study we show that LSTM is also a good mechanism for learning to compose music. We present experimental results showing that LSTM successfully learns a form of blues music and is able to compose novel (and we believe pleasing) melodies in that style. Remarkably, once the network has found the relevant structure it does not drift from it. LSTM is able to play the blues with good timing and proper structure as long as one is willing to listen.},
    author = {Eck, D. and Schmidhuber, J.},
    doi = {10.1109/NNSP.2002.1030094},
    file = {:home/fliang/Documents/15-16/cambridge/research/papers/ml{\_}compositioin/2002{\_}ieee.pdf:pdf},
    isbn = {0780376161},
    issn = {0780376161},
    journal = {Neural Networks for Signal Processing - Proceedings of the IEEE Workshop},
    pages = {747--756},
    title = {{Finding temporal structure in music: Blues improvisation with LSTM recurrent networks}},
    volume = {2002-Janua},
    year = {2002}
}
@article{Tymoczko2009,
    abstract = {This paper considers three conceptions of musical distance (or inverse “similarity”) that produce three different musico-geometrical spaces: the first, based on voice leading, yields a collection of continuous quotient spaces or orbifolds; the second, based on acoustics, gives rise to the Tonnetz and related “tuning lattices”; while the third, based on the total interval content of a group of notes, generates a six-dimensional “quality space” first described by Ian Quinn. I will show that although these three measures are in principle quite distinct, they are in practice surprisingly interrelated. This produces the challenge of determining which model is appropriate to a given music-theoretical circumstance. Since the different models can yield comparable results, unwary theorists could potentially find themselves using one type of structure (such as a tuning lattice) to investigate properties more perspicuously represented by another (for instance, voice-leading relationships).},
    author = {Tymoczko, Dmitri},
    doi = {10.1007/978-3-642-02394-1_24},
    file = {:home/fliang/Documents/15-16/cambridge/research/papers/distance.pdf:pdf},
    isbn = {9783642023934},
    issn = {18650929},
    journal = {Communications in Computer and Information Science},
    keywords = {Fourier transform,Orbifold,Tonnetz,Tuning lattice,Voice leading},
    pages = {258--272},
    title = {{Three conceptions of musical distance}},
    volume = {38},
    year = {2009}
}
@article{Liu2014,
    abstract = {We propose a framework for computer music composition that uses resilient propagation (RProp) and long short term memory (LSTM) recurrent neural network. In this paper, we show that LSTM network learns the structure and characteristics of music pieces properly by demonstrating its ability to recreate music. We also show that predicting existing music using RProp outperforms Back propagation through time (BPTT).},
    archivePrefix = {arXiv},
    arxivId = {1412.3191},
    author = {Liu, I-Ting and Ramakrishnan, Bhiksha},
    eprint = {1412.3191},
    file = {:home/fliang/Documents/15-16/cambridge/research/papers/1412.3191.pdf:pdf},
    journal = {arXiv:1412.3191},
    pages = {1--9},
    title = {{Bach in 2014: Music Composition with Recurrent Neural Network}},
    url = {http://arxiv.org/abs/1412.3191},
    volume = {5},
    year = {2014}
}
@article{Koutnik2014,
    abstract = {Sequence prediction and classification are ubiquitous and challenging problems in machine learning that can require identifying complex dependencies between temporally distant inputs. Recurrent Neural Networks (RNNs) have the ability, in theory, to cope with these temporal dependencies by virtue of the short-term memory implemented by their recurrent (feedback) connections. However, in practice they are difficult to train successfully when long-term memory is required. This paper introduces a simple, yet powerful modification to the simple RNN (SRN) architecture, the Clockwork RNN (CW-RNN), in which the hidden layer is partitioned into separate modules, each processing inputs at its own temporal granularity, making computations only at its prescribed clock rate. Rather than making the standard RNN models more complex, CW-RNN reduces the number of SRN parameters, improves the performance significantly in the tasks tested, and speeds up the network evaluation. The network is demonstrated in preliminary experiments involving three tasks: audio signal generation, TIMIT spoken word classification, where it outperforms both SRN and LSTM networks, and online handwriting recognition, where it outperforms SRNs.},
    archivePrefix = {arXiv},
    arxivId = {arXiv:1402.3511v1},
    author = {Koutnik, Jan and Greff, Klaus and Gomez, Faustino and Schmidhuber, Juergen},
    eprint = {arXiv:1402.3511v1},
    file = {:home/fliang/Documents/15-16/cambridge/research/papers/iran{\_}tsobrien{\_}cs224d{\_}proposal.pdf:pdf},
    isbn = {9781634393973},
    journal = {Proceedings of The 31st International Conference on Machine Learning},
    pages = {1863--1871},
    title = {{A Clockwork RNN}},
    url = {http://jmlr.org/proceedings/papers/v32/koutnik14.html},
    volume = {32},
    year = {2014}
}
@article{Cuthbert2011,
    author = {Cuthbert, Michael Scott and Cabal-ugaz, Jose and Ariza, Chris and Hadley, Beth},
    file = {:home/fliang/Documents/15-16/cambridge/research/papers/music{\_}classification/Cuthbert{\_}Ariza{\_}Cabal-Ugaz{\_}Hadley{\_}Parikh-Hidden-NIPS2011.pdf:pdf},
    journal = {Proceedings of the Neural Information Processing Systems Conference},
    pages = {3--4},
    title = {{Hidden Beyond MIDI's Reach : Feature Extraction and Machine Learning with Rich Symbolic Formats in music21}},
    year = {2011}
}
@article{Scott2015,
    title={music21: A toolkit for computer-aided musicology and symbolic music data},
    author={Cuthbert, Michael Scott and Ariza, Christopher},
    year={2010},
    publisher={International Society for Music Information Retrieval}
}
@article{Mikolov2015,
    abstract = {Recurrent neural network is a powerful model that learns temporal patterns in sequential data. For a long time, it was believed that recurrent networks are difficult to train using simple optimizers, such as stochastic gradient descent, due to the so-called vanishing gradient problem. In this paper, we show that learning longer term patterns in real data, such as in natural language, is perfectly possible using gradient descent. This is achieved by using a slight structural modification of the simple recurrent neural network architecture. We encourage some of the hidden units to change their state slowly by making part of the recurrent weight matrix close to identity, thus forming a kind of longer term memory. We evaluate our model on language modeling tasks on benchmark datasets, where we obtain similar performance to the much more complex Long Short Term Memory (LSTM) networks (Hochreiter {\&} Schmidhuber, 1997).},
    archivePrefix = {arXiv},
    arxivId = {arXiv:1412.7753v1},
    author = {Mikolov, Tomas and Joulin, Armand and Chopra, Sumit and Mathieu, Michael},
    eprint = {arXiv:1412.7753v1},
    file = {:home/fliang/Documents/reading/rnn-lstm/1412.7753.pdf:pdf},
    journal = {Iclr},
    pages = {1--9},
    title = {{Learning Longer Memory in Recurrent Neural Networks}},
    url = {http://arxiv.org/pdf/1412.7753v1.pdf},
    year = {2015}
}
@article{Herlands2014,
    author = {Herlands, William and Der, Ricky and Greenberg, Y and Levin, S},
    file = {:home/fliang/Documents/15-16/cambridge/research/papers/8314-38152-1-PB.pdf:pdf},
    isbn = {9781577356776},
    journal = {Twenty-Eighth AAAI Conference on Artificial Intelligence},
    keywords = {Applications},
    mendeley-groups = {Thesis,Thesis/Feature Engineering},
    pages = {276--282},
    title = {{A Machine Learning Approach to Musically Meaningful Homogeneous Style Classification}},
    url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/view/8314},
    year = {2014}
}
@article{Pascanu2012,
    abstract = {There are two widely known issues with prop- erly training Recurrent Neural Networks, the vanishing and the exploding gradient prob- lems detailed in Bengio et al. (1994). In this paper we attempt to improve the under- standing of the underlying issues by explor- ing these problems from an analytical, a geo- metric and a dynamical systems perspective. Our analysis is used to justify a simple yet ef- fective solution. We propose a gradient norm clipping strategy to deal with exploding gra- dients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
    archivePrefix = {arXiv},
    arxivId = {arXiv:1211.5063v2},
    author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
    doi = {10.1109/72.279181},
    eprint = {arXiv:1211.5063v2},
    file = {:home/fliang/Documents/reading/rnn-lstm/1211.5063v2.pdf:pdf},
    isbn = {08997667 (ISSN)},
    issn = {1045-9227},
    journal = {Proceedings of The 30th International Conference on Machine Learning},
    mendeley-groups = {Thesis},
    number = {2},
    pages = {1310--1318},
    pmid = {18267787},
    title = {{On the difficulty of training recurrent neural networks}},
    url = {http://jmlr.org/proceedings/papers/v28/pascanu13.pdf},
    year = {2012}
}
@article{Bengio1994,
    abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.},
    archivePrefix = {arXiv},
    arxivId = {arXiv:1211.5063v2},
    author = {Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
    doi = {10.1109/72.279181},
    eprint = {arXiv:1211.5063v2},
    file = {:home/fliang/Documents/reading/rnn-lstm/1211.5063v1.pdf:pdf},
    isbn = {1045-9227 VO  - 5},
    issn = {19410093},
    journal = {IEEE Transactions on Neural Networks},
    number = {2},
    pages = {157--166},
    pmid = {18267787},
    title = {{Learning Long-Term Dependencies with Gradient Descent is Difficult}},
    url = {http://jmlr.org/proceedings/papers/v28/pascanu13.pdf},
    volume = {5},
    year = {1994}
}
@article{Cybenko1993,
    abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
    author = {Cybenko, George},
    doi = {10.1007/BF02836480},
    file = {:home/fliang/Documents/reading/art{\%}3A10.1007{\%}2FBF02551274.pdf:pdf},
    isbn = {0780300564},
    issn = {10009221},
    journal = {Approximation Theory and its Applications},
    keywords = {approximation,completeness,neural networks},
    number = {3},
    pages = {17--28},
    title = {{Degree of approximation by superpositions of a sigmoidal function}},
    volume = {9},
    year = {1993}
}
@article{Bengio2011,
    abstract = {Deep architectures are families of functions corresponding to deep circuits. Deep Learning algorithms are based on parametrizing such circuits and tuning their parameters so as to approximately optimize some training objective. Whereas it was thought too difficult to train deep architectures, several successful algorithms have been proposed in recent years. We review some of the theoretical motivations for deep architectures, as well as some of their practical successes, and propose directions of investigations to address some of the remaining challenges},
    archivePrefix = {arXiv},
    arxivId = {1206.5533},
    author = {Bengio, Yoshua and Delalleau, Olivier},
    doi = {10.1007/978-3-642-24412-4_3},
    eprint = {1206.5533},
    file = {:home/fliang/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio, Delalleau - 2011 - On the expressive power of deep architectures.pdf:pdf},
    isbn = {9783642244117},
    issn = {03029743},
    journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    pages = {18--36},
    pmid = {25497547},
    title = {{On the expressive power of deep architectures}},
    volume = {6925 LNAI},
    year = {2011}
}
@article{At,
    author = {At, Fakult},
    file = {:home/fliang/Documents/reading/rnn-lstm/10.1.1.49.1968.pdf:pdf},
    title = {{¨ F UR ¨ INFORMATIK der Technischen Universit at Learning Task-Dependent Distributed Representations by Backpropagation Through Structure Report AR-95-02 Institut fur Informatik Christoph Goller Andreas Kuchler Computer Science University of Ulm}}
}
@article{:/content/asa/journal/jasa/55/5/10.1121/1.1914648,
    author = "Terhardt, Ernst",
    title = "Pitch, consonance, and harmony",
    journal = "The Journal of the Acoustical Society of America",
    year = "1974",
    volume = "55",
    number = "5",
    pages = "1061-1069",
    url = "http://scitation.aip.org/content/asa/journal/jasa/55/5/10.1121/1.1914648",
    doi = "http://dx.doi.org/10.1121/1.1914648"
}
@article{denton1997history,
    title={The History of Musical Tuning and Temperament during the Classical and Romantic Periods},
    author={Denton, Christine and Fillion, M},
    year={1997}
}
@online{spn,
    author = {Flutopedia},
    title = {Octave Notation},
    year = 2016,
    url = {http://www.flutopedia.com/octave_notation.htm},
    urldate = {2016-07-25}
}
@article{jordan1997serial,
    title={Serial order: A parallel distributed processing approach},
    author={Jordan, Michael I},
    journal={Advances in psychology},
    volume={121},
    pages={471--495},
    year={1997},
    publisher={Elsevier}
}
@article{elman1990finding,
    title={Finding structure in time},
    author={Elman, Jeffrey L},
    journal={Cognitive science},
    volume={14},
    number={2},
    pages={179--211},
    year={1990},
    publisher={Elsevier}
}
