\documentclass[dissertation.tex]{subfiles}
\begin{document}

\chapter{Related Work}
% This chapter covers relevant (and typically, recent) research
% which you build upon (or improve upon). There are two complementary
% goals for this chapter:
% \begin{enumerate}
%   \item to show that you know and understand the state of the art; and
%   \item to put your work in context
% \end{enumerate}

% Ideally you can tackle both together by providing a critique of
% related work, and describing what is insufficient (and how you do
% better!)

% The related work chapter should usually come either near the front or
% near the back of the dissertation. The advantage of the former is that
% you get to build the argument for why your work is important before
% presenting your solution(s) in later chapters; the advantage of the
% latter is that don't have to forward reference to your solution too
% much. The correct choice will depend on what you're writing up, and
% your own personal preference.

% Some common themes
% \begin{enumerate}
%   \item Use of domain-specific representations for musical data
%   \item Modeling at multiple resolutions / timescales (i.e. chords vs notes)
% \end{enumerate}
% In contrast, we avoid imposing prior knowledge in order to avoid any biases
% and hope that the model will learn the features relevant for good performance.

\section{Machine learning on musical data}

Computational methods applied to large corpora of music was first described in
\cite{coutinho2005computational}, which termed the phrase ``computational
musicology.'' Since then, development modern tools have greatly aided research
efforts. \texttt{music21} \cite{Scott2015} is a Python programming environment
for performing computations over musical composition data which has been
utilized for a variety of computational musicology tasks ranging from
hierarchical modelling of metrical structure \cite{ariza2010modeling}, feature
generation for downstream machine learning\cite{Cuthbert2011}, and style
classification \cite{Herlands2014}.

Focusing on machine learning applications, most research can be
classified under one or more of the following tasks:
\begin{enumerate}
  \item Classification: the style, composer, or other musical attribute is to be classified
  \item harmonization: a melody is given and the remaining parts are to be generated
  \item Completion: given the beginning of a score, the remainder is to be generated
  \item Automatic Composition: a complete unconstrained score is to be generated
\end{enumerate}

There is a vast body of research dealing with music classification tasks,
including: style classification \cite{Herlands2014}
\cite{dannenberg1997machine}, automated harmonic analysis \cite{ni2012end},
information retrieval \cite{mandel2006support}, and performer identification
\cite{stamatatos2005automatic}. However, it is not straightforward to utilize
work in this area to solve our research goals of music synthesis\todo{of what?}.

\section{Models for music synthesis}

Unlike classification, the other tasks (harmonization, completion, automatic
composition) require synthesis of novel music. One broad classification of
music synthesis methods \cite{toiviainen2000symbolic} distinguishes between
symbolic (i.e. rule based) versus connectionist (i.e. neural networks).

\subsection{Symbolic rule-based methods}

CHORAL \cite{ebciouglu1988expert} is one of the first rule based expert system
for harmonising Bach chorales. It uses 350 manually defined rules as well as
hand-tuned search heuristics. A later system called \emph{Experiments in Music
Intelligence} (EMI) \cite{cope1992computer} automatically extracted rules to
build an augmented transition network\cite{wanner1980atn}. In
\cite{cruz1998learning}, grammatical inference is used to learn regular
grammars over chord progressions for modelling musical style.
\cite{tsang1991harmonizing} applies constraint logic programming for generating
harmonizations which satisfy certain harmonic constraints.

While symbolic methods can easily incorporate domain-specific knowledge and are
more interpretable than connectionist models, they are also inherently biased
by their creators' subjective theories on harmony and music cognition.
Furthermore, specification of hand-crafted rules is a laborious process which
requires significant music experience and does not improve when given larger
amounts of data. Additionally, rule-based methods are brittle to distortion and
noise and limit creativity by disallowing deviation from the defined rules.

\subsection{Connectionist methods}

Neural networks have been previously applied to music with varying degrees of
success\cite{griffith1999musical}. The earliest connectionist music models
utilized note-level Jordan RNNs on melody generation and harmonization tasks
\cite{todd1988sequential} \cite{todd1989connectionist}
\cite{bharucha1989modeling}. \todo{say more}

A landmark connectionist system is Mozer's CONCERT \cite{mozer1994neural}, a
BPTT RNN for note-by-note composition. CONCERT models music at two
levels of resolution: notes and chords. Notes utilize a psychologically-based
representation \cite{shepard1982geometrical} and chords use a distributed
embedding originally trained for style classification
\cite{laden1989representation}. The model passes objective evaluations by
faithfully reproducing scales but ``while the local contours made sense, the
pieces were not musically coherent, lacking thematic structure and having
minimal phrase structure and rhythmic organisation`` \cite{mozer1994neural}.

\cite{Boulanger-Lewandowski2012} proposed the RNN-RBM. a time-varying RBM with
hidden units evolving over time according to a RNN, to model polyphonic music
on a piano roll representation. However, training the RNN-RBM requires an
expensive contrastive divergence sampling step at each timestep and a
nontrivial Hessian-free optimisation routine. Furthermore, the authors
quantized music to eighth-notes. In contrast, our work uses the well-understood
truncated BPTT algorithm for training and quantizes to sixteenth-notes to achieve
two-times higher time resolution.

\cite{Lyu2015} extended the RNN-RBM\cite{Boulanger-Lewandowski2012} to use a
LSTM instead of a RNN for modelling hidden unit time dynamics. Unfortunately,
the authors do not evaluate their model beyond stating ``LSTM-RBM could learn
melody lines\ldots while RNN-RBM generates inconsistent and unpleasant sample
sequences.''

\subsection{Hybrid methods}

Hybrid approaches which combine both rule-based and connectionist methods have
also been investigated. One of the first hybrid systems for music synthesis is
HARMONET \cite{hild1991harmonet}, which combines connectionist neural networks
with formal rules to specifically harmonize Bach chorales. It implements a
domain-specific processing pipeline consisting of:
\begin{enumerate}
  \item Harmonic modelling: Predict harmonic skeleton (i.e. Roman numerals
  quantized to quarter-notes) using a neural network
  \item Expand each Roman numeral to chords using formal rules
  \item Ornamentation: add eighth-notes using formal rules
\end{enumerate}
The specialized architecture of HARMONET makes it unable to generalize
to other tasks such as automatic composition or composition scoring\todo{Define this task}.
Additionally, the use of formal rules makes the system suffer from the same
problems that rule-based systems suffer from.

MELONET \cite{feulner1994melonet} builds on top of HARMONET's harmonic
modelling to construct chorale partitas (i.e. variations where one of the
parts is varied in a harmonically believable way). It first introduced multiple
ideas which have been rediscovered in recent years, including:
\begin{enumerate}
  \item Delayed update units to model multiple timescales (described again in Clockwork
  RNNs \cite{Koutnik2014})
  \item Use of Resilient Propagation (RProp) \cite{riedmiller1993direct} for training (
  described again in \cite{Liu2014})
\end{enumerate}
Additionally, MELONET utilizes a motif classification neural network to
explicitly force motifs to appear multiple times within a partita.
Follow up work \cite{hornel1996learning} extends MELONET to use a distributed
representation for motifs and a genetic algorithm for training.
While MELONET introduces many novel ideas, its limited training set size
(16 Pachelbel chorales \cite{hornel1997melonet}) and domain-specific
architecture limit the generalizability of results. \todo{Better analyze this}

CHIME \cite{franklin2001learning} adopted the Jordan RNN from
\cite{todd1989connectionist} to add a second training phase using actor-critic
reinforcement learning \cite{sutton1998reinforcement}. The critic is
constructed using a collection of ``music rules,'' enabling incorporation of
prior knowledge.

\subsection{LSTM music synthesis models}

Prior work has demonstrated LSTM possesses many properties desirable for music
applications. Their superiority over traditional RNNs has been well
documented\cite{gers2001lstm}. They can learn to count and measure time
intervals between events spaced arbitrarily far apart in time
\cite{gers2000recurrent}, a property $N$-gram language models do not possess.
\cite{gers2002learning} demonstrated LSTM learning to produce self-sustaining
oscillations at a regular frequency, suggesting that they are capable of
discovering periodic structure. \cite{franklin2006recurrent} evaluates various
RNN architectures on variety of music tasks and concludes: ``while we have
found a task that challenges a single LSTM network, we do not believe that any
other recurrent networks we have used would be able to learn these songs.'''

One of the first applications of LSTMs to music was to improvise blues melody
lines \cite{Eck2002}\cite{Eck2002-blues}. Using a LSTM to model blues chord
progressions and another to model melody lines given chords, the authors
reported that LSTM can learn long term music structure such as repeated motifs
can be learned without explicit modelling (e.g. MELONET). However, the music
representation quantized to eighth notes, used considered pitch classes without
accounting for octaves, and limited the model to $12$ possible chords.
Additionally, there was ``no explicit way to determine when a note ends,''
prohibiting discrimination between four consecutive articulations of a note at
the same pitch from a single note held for four timesteps. In contrast, our
model accounts for the octaves in addition to pitch class, does not restrict
the possible chords, operates at twice the time resolution, and also
models when a note ends.

More recently, \cite{sturm2015folk} \cite{sturm2016music} trained
character-level LSTMs on 23,000 folk music scores represented in ABC
notation\cite{abcstandard}, a high-level text format for music. ABC format is
unsatisfactory for our use case because polyphonic scores are encoded one part
at a time so notes sounding close together in time may appear very far apart in
the sequence. As a result, it is unsurprising that the authors do not
explicitly address polyphony and present exclusively monophonic results.

Many variants of the LSTM architecture have been proposed. Perhaps the most
well known is the gated recurrent unit (GRU)\cite{cho2014learning}, which
constrains the input and forget gates to sum to $1$. \cite{Mikolov2015}
proposed the structurally constrained RNN (SCRN), a simple architecture
achieving comparable performance to LSTMs. Of most relevance to music,
\cite{Koutnik2014} proposed the clockwork RNN for explicitly modelling phenomena
occurring at multiple timescales by updating different blocks of the hidden
state at different periods. Whether these differences matter is not definitive:
\cite{greff2015lstm} performed 5400 experiments on eight different
architectures and found no significant difference in performance compared to
the original LSTM architecture. \cite{Nayebi2015} reports LSTMs significantly
outperform GRUs in music applications.

\section{Generative modelling of Bach Chorales}

One of the first generative models for harmonizing Bach chorales is Bellgard
and Tsaing's effective Boltzmann machine model \cite{bellgard1994harmonizing}. Their model
uses Boltzmann machines to enforce consistency within local contexts. As a result,
their model is unable to capture long-range dependencies. Furthermore, they quantize
to half-notes and only achieve $1/8$ the time resolution of our model.

\cite{Allan2005} used HMMs to harmonize Bach chorales. Their model consists of
two separate HMM models: one for generating harmonizations and another for
ornamentation. Their model uses a discrete harmonic encoding of chords for
hidden states. In contrast, our model uses an unconstrained continuous hidden
state and requires no separate ornamentation step.

\cite{Liu2014} applied LSTMs to Bach chorales and reports significant gains
using RProp instead of BPTT, a technique previously utilized by
MELONET\cite{feulner1994melonet}. However, they erroneously use a mean squared
error training criterion for a classification task, casting doubts on the
validity of their experiments.

\cite{Brien2016} compared RNN models for Bach chorales and found clockwork RNNs to
yield the lowest validation loss. However, their data format does not permit
independent articulation of parts. More importantly, the performance margin between
clockwork RNNs and LSTMs was very small ($6.5$ vs $6.75$ cross-entropy loss) and their
implementation resets the LSTM state when truncating gradients during BPTT, limiting the
time-range of learned dynamics to be at most the sequence length.

\todo{Tom Collins work}

\printbibliography

\end{document}
