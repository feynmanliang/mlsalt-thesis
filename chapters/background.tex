\chapter{Background}

% A more extensive coverage of what's required to understand your
% work. In general you should assume the reader has a good undergraduate
% degree in computer science, but is not necessarily an expert in
% the particular area you've been working on. Hence this chapter
% may need to summarize some ``text book'' material.

% This is not something you'd normally require in an academic paper,
% and it may not be appropriate for your particular circumstances.
% Indeed, in some cases it's possible to cover all of the ``background''
% material either in the introduction or at appropriate places in
% the rest of the dissertation.

This chapter reviews background relevant to the remainder of this dissertaion.
The scope includes an overview of music theory and automatic composition
and also includes coverage of sequence probability models.

\section{Music theory}

Music theory a branch of musicology concerned with the study of the rules and
practices of music. While the general field includes study of acoustic
qualities such as timbre and waveform synthesis, our work is concerned with
modelling the musical composition itself rather than acoustic features. This is
justified because acoustic features are more closely related to a particular
reproduction (e.g. the skill of the performers, quality of the instruments)
and are likely to vary significantly across different performances. Indeed,
references to a piece of music generally refer to the underlying composition
itself rather than any particular performance of the piece. This suggests
that the composition itself is more significant and hence a more desirable
modeling target.

We refer refer to a musical composition as a \textbf{score}, which we formally
define to be an ordered sequence of \textbf{note}s. Each note possesses a pitch
(i.e.\ frequency) and a time interval (i.e.\ start and stop times) and
represents an instruction for the performer to articulate the given pitch at
the start time and hold the note until the stop time. Multiple notes may
overlap in time, in which case all notes are to be played simultaneously.

\subsection{Pitches and tuning systems}

The \textbf{pitch} of a note refers to the frequency of sound used in its
reproduction and the difference between two pitches is called a \textbf{pitch
interval}. A fundamental pitch interval in Western music is the
\textbf{octave}, which is defined to be the interval between a frequency and
its double.

While in theory an uncoutable number of pitches are available because
frequencies are real-valued and hence there are uncountably many, tuning
systems are oftentimes employed to restrict the set of available pitches in
order to simplify composition, analysis, and reproduction. A \textbf{tuning
system} restricts the available pitch intervals, limiting the possible pitches
to a discrete set.

Western music commonly uses a tuning system called \textbf{12-TET} (12 tone
equal temperament) tuning, which divides an octave into 12 \textbf{pitch
classes} ($C$, $C\#/D\flat$, $D$, $D\#/E\flat$, $E$, $F$, $F\#/G\flat$, $G$,
$G\#/A\flat$, $A$, $A\#/B\flat$, $B$) all equally spaced on a logarithmic
scale. Each pitch class represents an equivalence class of frequencies which
all differ by an integral number of octaves (i.e. are doublings/halvings of
each other). This means that unambiguously specifying a pitch requires
specifying both a pitch class and an octave. Common practice numbers octaves
with respect to a standard sized keyboard, where the key corresponding to the
\textbf{middle $C$} note is referred to as $C_5$ because it is $5$ octaves
above the lowest $C$ key. The interval between two adjacent pitch classesis
called a \textbf{half step} or \textbf{half tone} and two half steps are called
a \textbf{whole step/tone}.

In our work, we restrict attention to 12-TET tuning because it is the most
common tuning system employed by Western composers over the last 

Note that while 12-TET restricts the available pitches to a discrete set, it
lacks specification of a reference pitch frequency and hence does not yield a
correspondance between pitches and frequencies. This degree of freedom gives
rise to \textbf{transposition invariance}: a score of music can be offset by a
constant frequency without altering characteristics arising from the pitches
present in the music. In modern practice, the general tuning standard of $A440$
defines a reference frequency of 440 Hz for the $A$ pitch class directly above
middle C.

\subsection{Tonal music}

Tonal music is characterized by the prevalence of one pitch class (the
\textbf{tonic}) around which the melody and harmony are built. 

A basic concept
within tonal music is the \textbf{scale} which defines a subset of pitch
classes that are ``in key'' with respect to the tonic. Two fundamental scales
are the major (with step pattern whole-whole-half-whole-whole-whole-half) and
minor scales (whole-half-whole-whole-half-whole-whole). The choice of tonic and
scale is referred to as the \textbf{key}and a change in key during a piece is
called a \textbf{modulation}Many musical phenomena such as stability,
expectation, and resolution can be attributed to tonal characteristics.

The \textbf{tempo} of a piece refers to its speed or pace and is measured by
beats per minute. In 4/4 time signature, a \textbf{quarter note} or
\textbf{crotchet} denotes the time interval between two beats. In addition to
pitch quantization, durations are also commonly quantized to subdivisions and
multiples of a crotchet.

\subsection{Notation}

We consider note duration, time, and velocity. We neglect changes in timing
(e.g. ritardandos), dynamics (e.g. crescendos), and stylistic notations (e.g.
accents, staccatos, legatos).

\emph{Piano roll} music transcriptions are quantized both in time ($t \in T$)
and note frequencies ($n \in N$). frequencies quantized to a piano roll.
\todo{Motivate quantization with Western music}.

We can represent a piano roll transcription as a high-dimensional vecctor
$X_{t,n} \in \RR^{|T| \times |N|}$ where $X_{t,n}$ denotes the note
velocities for note $n$ at time $t$.

\section{Automatic composition}

Algorithmic composition is the application of a well-defined algorithmic
procedure to compose music.

An interesting question regarding creativity: if an algorithm faithfully reproduces
an artist's creative process, what is the difference between music produced by the artist
and music produced by the algorithm?

\section{Sequence probability modelling}

Generating a "Bach-like" piece of music can be understood as drawing a random
sample from a distribution over musical scores which is statistically similar
to Bach's own compositions. Thus, we interpret the problem as one of
\emph{categorical sequence modeling}.

This type of problem has been well studied. In speech recognition, language
models parameterizing distributions over sentences are used as priors to refine
transcriptions.

However, since our model has to be able to generate Bach, we must be able to
sample from it. This rules out a broad class of sequence models, including
back-off N-grams and other interpolated language models.

Fortunately, low order N-grams and standard HMM-based models are sampleable and
thus can be used as baselines.

\subsection{Feedforward neural networks}

Feedforward neural networks are a parametric model capable of approximating a
broad range of functions \todo{Include universal approximation}. Their capacity
to model a diverse range of functions make them an appealing model for complex
unknown probability distributions.

A feedforward neural network is the composition of a sequence of layers, each
taking some \textbf{inputs} $\vec{x}$ and performs an affine transformation $\vec{z} =
\matr{W} [\vec{x}, 1]^\tp$ to yield \textbf{activations}. The activations are then
passed through an \textbf{activation function}, which applies an elementwise
nonlinearity $\sigma : \RR \to [L,U]$ squashing the outputs to into some connected bounded
subset $[L,U] \subset \RR$. A common choice is the logistic function $\sigma(z)
= \frac{1}{1+\exp{-z}}$, which squashes $\vec{y} \in [0, 1]$. Other choices
include $\sigma = \tanh$, in which case $[L, U] = [-1, 1]$.

\begin{figure}[htpb]
    \centering
    \input{Figures/nn-single.pdf_tex}
    \caption{Single neuron}
    \label{fig:nn-single}
\end{figure}

It is common to represent feedforward neural networks as directed acyclic
graphs (\autoref{fig:nn-layer}). Here, each node denotes a data value and
an edge from $s$ to $t$ notates that the value at $s$ is used to compute the
value at $t$.

\begin{figure}[htpb]
    \centering
    %\input{Figures/nn-ffw.pdf_tex}
    \caption{Feedforward Neural Network}
    \label{fig:nn-ffw}
\end{figure}

% \begin{figure}[htpb]
%     \centering
%     \input{Figures/nn-layer.tikz}
%     \caption{Single feedfoward neural network layer}
%     \label{fig:nn-layer}
% \end{figure}

Multiple layers can be composed together by treating the outputs from the previous layer
as the inputs to the next layer. \autoref{fig:ffw-nn} illustrates this on a 2-layer
feedforward neural network where the outputs of the first layer are used as the
inputs to the second layer (i.e. $x^{(1)} = y^{(0)}$).


% \begin{figure}[htbp]
%     \centering
%     \input{Figures/ffw-nn.tikz}
%     \caption{2-layer feedforward neural network}
%     \label{fig:ffw-nn}
% \end{figure}

\todo{Plug to deep learning}

\subsubsection{Modeling probability distributions}

A neural network can be used to model the distribution of a categorical random
variable $o$ by treating the final layer activations $\vec{z}^{(L)}$ as the
energies of a Boltzmann distribution (i.e.\ $\softmax$). This implies a
probability mass function on $o$ given by \autoref{eq:softmax}.

\begin{equation}
    \label{eq:softmax}
    P(o = k | \vec{z}^{(L)}) = \frac{\exp{-z^{(L)}_k}}{ \sum_{j} \exp{-z^{(L)}_j} }
\end{equation}

\subsubsection{Training with back-propogation}

Training neural networks is achieved using gradient descent methods, which
optimize parameters $\theta = \{W^{(i)} : 1 \leq i \leq L \}$ to minimize some
loss function $L(\vec{z}^{(L)}_{1:N}, \hat{o}_{1:N})$ between the network
outputs $\vec{z}^{(L)}_{1:N}$ and the true labels $\hat{o}_{1:N}$. For
probabilistic classification, a common choice is to assume independence
across training examples and use \textbf{cross-entropy loss}
(\autoref{eq:cross-entropy-loss}):

\begin{align}
    L(\vec{z}^{(L)}_{1:N}, \hat{o}_{1:N})
    &= \sum_{i=1}^{N} L(\vec{z}^{(L)}_i, \hat{o}_i) &\mbox{Independence across samples} \nonumber\\
    &= \sum_i \sum_k \delta_{\hat{o}_i,k} \log \frac{1}{P(o=k | \vec{y}_i^{(L)})} & \label{eq:cross-entropy-loss}
\end{align}

Gradient descent proceeds by using the Jacobian (i.e.\ gradient) $\nabla_\theta
L(\vec{z}^{(L)}_{1:N}, \hat{o}_{1:N})$ to iteratively update the network
parameters using successive first-order approximations (\autoref{eq:nn-training-iteration-scheme}).

\begin{align}
    \label{eq:nn-training-iteration-scheme}
    \theta^{(t+1)} = \theta^{(t)}
    - \eta_t \left[ \nabla_\theta L(\vec{z}^{(L)}_{1:N}, \hat{o}_{1:N}) \right]_{\theta = \theta^{(t)}}
\end{align}

Variants of \autoref{eq:nn-training-iteration-scheme} which adaptively set the
step size $\eta_t$ or incorporate/estimate the Hessian $\nabla^2_{\theta}
L(\cdot, \cdot)$ can yield performance when applied to neural network training.
However, their discussion is out of scope. \todo{Discuss RMSprop?}

To apply \autoref{eq:nn-training-iteration-scheme}, the gradient $\nabla_\theta
L(\vec{z}^{(L)}_{1:N}, \hat{o}_{1:N})$ must be computed. This can be
accomplished using \textbf{backpropogation} \todo{cite}, an algorithm which
exploits the independence structure to avoid unnecessary computations and make
gradient computations tractable.

Let $\delta^{(l)}_{j} = \frac{\pd L(\vec{z}^{(L)}_{1:N}, \hat{o}_{1:N})}{\pd
z^{(l)}_j}$ be the partial derivative of the loss with respect to the $j$th
activation of layer $l$. For the final $L$th layer, cross-entropy loss
with a Boltzmann distribution yields

\begin{align*}
    \delta^{(L)}_{j}
    &= - \sum_{i=1}^{N} \sum_{k} \frac{\pd}{\pd z^{(L)}_j} \delta_{\hat{o}_i,k} \log P(o=k | \vec{z}^{(L)}_i ) &\mbox{\autoref{eq:cross-entropy-loss}} \\
    &= \sum_{i=1}^{N} \left( P(o=k | \vec{z}^{(L)}_i ) - y_i \right) & \mbox{Softmax derivative}
\end{align*}

For earlier layers $l < L$, we have
\begin{align}
    \delta^{(l)}_{j}
    &= \sum_k \frac{\pd L(\vec{z}^{(L)}_{1:N}, \hat{o}_{1:N})}{\pd z^{(l+1)}_k}
    \frac{\pd z^{(l+1)}_k}{\pd z^{(l)}_j}\\
    &= \sum_k \delta^{(l+1)}_k
    \frac{\pd}{\pd z^{(l)}_j} \left( \matr{W}^{(l+1)} [\sigma(z^{(l)}), 1]^\tp \right)_k \\
    &= \sum_k \delta^{(l+1)}_k
    \matr{W}^{(l+1)}_{k,j} \sigma'(z^{(l)}_j) \label{eq:backprop}
\end{align}
This expression can be vectorized using the Hadamard product (elementwise multiplication), which
improves performance due to CPU cache locality and coalesced memory loads: \todo{DO THIS}
\begin{align}
    \circ
\end{align}
This recursion can be iterated until $l \to 0$.

The back-propogation algorithm consists of two steps:
\begin{enumerate}
    \item \emph{Forward pass}: Using current model parameters $\theta^{(t)}$,
        feed the data into the network to compute the activations $\vec{z}^{(l)}$,
        $1 \leq l \leq L$
    \item \emph{Backward pass}: Recursively iterate \autoref{eq:backprop}
        to compute $\vec{\delta}^{(l)}$, $1 \leq l \leq L$ using the activations
        $\vec{z}^{(l)}$ obtained from the forward pass
\end{enumerate}

After the backwards pass, gradients with respect to model parameters are easily obtained
\begin{align}
    \frac{\pd L}{\pd W^{(l)}_{i,j} }
    &= \sum_k \frac{\pd L}{\pd z^{(l+1)}_k} \frac{\pd z^{(l+1)}_k}{\pd W^{(l)}_{i,j}} \\
    &= \sum_k \delta^{(l+1)}_k z^{(l)}_j
\end{align}

Some appealing properties of backpropogation:
\begin{itemize}
    \item Efficient exploitation of the computation graph: chain rule expansions
        are constrained by the computation graph, improving efficiency
        because factors which don't contribute to a given $\delta^{(l)}$
        are neglected in the recursion
    \item Implementation using local rules: the forward/backward pass
        at any layer $l$ only requires knowledge of $z^{(l)}$, $\delta^{(l+1)}$,
        and the derivative of the activation $\sigma'$. As all these quantities are localized
        to one layer, this permits modular implementations where a node which can be back-propped
        through needs only implement a \texttt{forward()} method which computes activations
        given inputs and a \texttt{backward()} method which computes $\delta^{(l)}$ given
        activations.
\end{itemize}

\todo{Talk about how localization gives rise to computation graph and autodiff}

\subsection{Recurrent neural networks}

\begin{figure}[htpb]
    \centering
    \input{Figures/nn-rnn.pdf_tex}
    \caption{Recurrent neural network}
    \label{fig:nn-rnn}
\end{figure}


Recurrent neural networks (RNNs) are a generalization of feedforward neural
networks which introduce recurrent connections. This implies that its graphical
representation is no longer acyclic.

\begin{figure}[htpb]
    \centering
    \input{Figures/nn-rnn-elman.pdf_tex}
    \caption{Elman-type RNN}
    \label{fig:rnn-elman}
\end{figure}


Given an input $\vec{x}_{1:T}$, a RNN can be unrolled into an equivalent feed-forward
network with tied weights.

\begin{figure}[htpb]
    \centering
    \input{Figures/rnn-single-unrolled.pdf_tex}
    \caption{Unrolled single-layer RNN}
    \label{fig:rnn-single-unrolled}
\end{figure}

We can view the recurrent data $s_t$ as \textbf{state} which evolves
over consecutive \textbf{frames} $x_t$ according to dynamcis defined
by the RNN.

Mathematically, a RNN is defined by the equations
\begin{align}
    \s_t &= f(\U \x_t + \W \s_{t-1}) \\
    \o_t &= \softmax( \V \s_t)
\end{align}

$f$ is usually $\tanh$ or ReLU.

RNNs can be stacked:

\begin{figure}[htpb]
    \centering
    %\input{Figures/rnn-multi-unrolled.pdf_tex}
    \caption{Unrolled multi-layer RNN}
    \label{fig:rnn-multi-unrolled}
\end{figure}


The advantages of RNNs over feedforward networks include:
\begin{itemize}
    \item Ability to handle variable-length inputs: the RNN can be unrolled an arbitrary
        number of times to accomodate inputs $\vec{x}$ of different length
    \item Fixed dimension embeddings: after processing the entirety of an input
        sequence, the state of the RNN can be used as a fixed dimension embedding
        representing the input
    \item Sequential processing: the order of $\vec{x}_{1:T}$ will affect the state
        trajectory $s_{1:T}$, enabling the model to capture time-dependent dynamics
        within the input sequences
    \item Memory over time: the state $s \in \RR^D$ can take on an uncountably infinite
        number of values, allowing it to potentially act as memory which summarizes
        \emph{all} of the input up to the current time
\end{itemize}

\subsubsection{Comparison against HMMs}

Hidden Markov Models (HMMs) are another popular probabilistic model for
sequental data. \todo{Define HMMs}

While RNNs are similar HMMs in that both model the conditional distribution of
next frames given the previous context. However, RNNs additionally pass along
"hidden state" which summarizes contextual information from a potentially
infinite context window.

\subsubsection{Vanishing gradients}

In practice, it is observed that the hidden state does not capture long range
dependencies well and tend to suffer from vanishing/exploding gradient during
training. LSTMs are an improved RNN architecture which solve both of these
problems by introducing gates on the inputs, hidden state, and outputs. GRUs are
a variation of LSTMs which ties the weights to the input and forget gates.

To understanding the vanishing gradient problem, consider a simple recurrent network

\begin{eqnarray}
    h_t &=& \theta \phi(h_{t-1}) + \theta_x x_t \label{eq:ht-from-ht-1}\\
    y_t &=& \theta_y \phi(h_t)
\end{eqnarray}

After unrolling, the gradient of the error is given as a sum of the gradients
at each individual timestep

\begin{equation}
    \frac{\pd E}{\pd \theta} = \sum_{t=1}^S \frac{\pd E_t}{\pd \theta}
\end{equation}

Since the current hidden state $h_t$ depends on all prior hidden states $h_k$ ($k < t$),
the chain rule yields

\begin{equation}
    \frac{\pd E_t}{\pd \theta} = \sum_{k=1}^t \frac{\pd E_t}{\pd y_t} \frac{\pd y_t}{\pd h_t} \frac{\pd h_t}{\pd h_k} \frac{\pd h_k}{\pd \theta}
\end{equation}

Noting that time dynamics occur only through $h_t$

\begin{equation}
    \label{eq:prod-hi}
    \frac{\pd h_t}{\pd h_k} = \prod_{i=k+1}^t \frac{\pd h_i}{\pd h_{i-1}}
\end{equation}

The factors in the product can be obtained by differentiating \autoref{eq:ht-from-ht-1}

\begin{equation}
    \frac{\pd h_i}{\pd h_{i-1}} = \theta^\tp \diag\left[ \phi'(h_{i-1}) \right]
\end{equation}

Let $\|\cdot\|$ be any submultiplicative matrix norm. For illustration purposes, we will use the
\textbf{operator norm} defined as
\begin{equation}
    \| A \| = \sup_{x \in \RR^n; x \neq 0} \frac{|A x|}{|x|}
\end{equation}
where $|\cdot|$ is the standard Euclidian norm. However, any other submultiplicative norm
(e.g. Frobenius, spectral, nuclear, Shatten $p$-norms) is equally valid.

From submultiplicativity, we have

\begin{equation}
    \left\| \frac{\pd h_i}{\pd h_{i-1}} \right\|
    \leq \|\theta^\tp\| \| \diag\left[ \phi'(h_{i-1}) \right] \|
    \leq \gamma_{\theta} \gamma_\phi
\end{equation}

where $\gamma_\theta \coloneqq \|\theta\|$ is the norm of the hidden dynamics matrix $\theta$
and
\begin{align}
    \gamma_\phi
    &\coloneqq \sup_{h \in \RR^n} \| \diag \left[ \phi'(h) \right] \|  &\\
    &= \sup_{h \in \RR^n} \max_i \phi'(h)_i &\mbox{Operator norm of diag} \\
    &= \sup_{t \in \RR} \phi'(t) &\mbox{$\phi$ operates elementwise}
\end{align}

For common activation functions $\gamma_{\phi} \leq \infty$ (e.g. $\gamma_\phi = 1$ for $\phi = \tanh$
and $\gamma_\phi = 1/4$ for $\phi = \sigma$).

This gives a bound on \autoref{eq:prod-hi}, namely

\begin{equation}
    \left\| \frac{\pd h_t}{\pd h_k} \right\|
    = \left\| \prod_{i=k+1}^{t} \frac{\pd h_t}{\pd h_k} \right\|
    \leq (\gamma_\theta \gamma_\phi)^{t-k}
\end{equation}

When $|\gamma_\theta \gamma_\phi| < 1$, these gradients exponentially vanish to zero as
the time interval $t - k$ increases, hindering the network's ability to learn long-range
dynamics.

A similar argument \cite{Bengio1994} illustrates an analogous exponential explosion
when $\|\theta\| > 1$ (the ``exploding gradient'' problem).

The solution is to rewrite \autoref{eq:ht-from-ht-1} such that
\autoref{eq:prod-hi} does not vanish/explode for large $t - k$.
One possibility would be

\begin{equation}
    h_t = h_{t-1} + \theta_x x_t
\end{equation}

However, this solution is unsatisfactory as all hidden state dynamics have been
removed.

\subsubsection{Long Short Term Memory}

LSTMs are a RNN architecture which mitigates vanishing/exploding gradients by constraining
the hidden state dynamics. It does so by constraining the nonlinearity $\phi = \matr{I}$ to be the
identity and introducing three gates:
\begin{itemize}
    \item \textbf{Input gate}: scales input $x_t$ elementwise by $i_t \in [0,1]$, affects how $h_t$ is written to
    \item \textbf{Output gate}: scales output $y_t$ elementwise by $o_t \in [0,1]$, affects how $h_t$ is read from
    \item \textbf{Forget gate}: scales previous cell value $h_{t-1}$ by $f_t \in [0,1]$, acts as a reset mechanism
\end{itemize}

\begin{figure}[htpb]
    \centering
    %\input{Figures/lstm-unit.pdf_tex}
    \caption{Single LSTM unit}
    \label{fig:lstm-unit}
\end{figure}

The equations for an LSTM are

\begin{eqnarray}
    i_t &=& \sigma(W_{xi} x_t + W_{yi} y_{t-1} + W_{ci} c_{t-1} + b_i) \\
    o_t &=& \sigma(W_{xo} x_t + W_{yo} y_{t-1} + W_{co} c_{t-1} + b_o) \\
    f_t &=& \sigma(W_{xf} x_t + W_{yf} y_{t-1} + W_{cf} c_{t-1} + b_f) \\
    h_t &=& f_t \odot h_{t-1} + i_t \odot \tanh(x_t W_{xc} + h_{t-1} W_{hc} + b_c) \label{eq:lstm-dynamics} \\
    y_t &=& o_t \odot \tanh(c_t)
\end{eqnarray}
where $\odot$ denotes elementwise multiplication of vectors.

Note that \autoref{eq:lstm-dynamics} has $\phi$ set to the identity function
hence $\gamma_\phi = 1$. This removal of the non-linearity in the hidden state
recursion is known as the \textbf{constant error carousel} \todo{cite}, which
allows the network to propogate errors without modification by $\varphi'(h_k)$.

The addition of the gates enables the model to learn when to read/write from
memory and when to reset, enabling longer-range dependencies to be learned when
an input is written to memory (i.e. $i_t$ high) and protected for a long
duration (i.e. $f_t \approx 1$ and $i_t \approx 0$).

Some authors define LSTMs such that $h_t$ is not used to compute gate activations,
referring to the case where $h_t$ is connected as ``peephole connections.'' In our
work, we use ``LSTM'' to refer to LSTMs with peephole connections.
