\chapter{Background}

% A more extensive coverage of what's required to understand your
% work. In general you should assume the reader has a good undergraduate
% degree in computer science, but is not necessarily an expert in
% the particular area you've been working on. Hence this chapter
% may need to summarize some ``text book'' material.

% This is not something you'd normally require in an academic paper,
% and it may not be appropriate for your particular circumstances.
% Indeed, in some cases it's possible to cover all of the ``background''
% material either in the introduction or at appropriate places in
% the rest of the dissertation.

This chapter reviews background relevant to the remainder of this dissertaion.
The scope includes an overview of music theory and automatic composition
and also includes coverage of sequence probability models.

\section{Music theory}

Music theory a branch of musicology concerned with the study of the rules and
practices of music. While the general field includes study of acoustic
qualities such as timbre and waveform synthesis, our work is concerned with
modelling the musical composition itself rather than acoustic features. This is
justified because acoustic features are more closely related to a particular
reproduction (e.g. the skill of the performers, quality of the instruments)
and are likely to vary significantly across different performances. Indeed,
references to a piece of music generally refer to the underlying composition
itself rather than any particular performance of the piece. This suggests
that the composition itself is more significant and hence a more desirable
modeling target.

We refer refer to a musical composition as a \textbf{score}, which we formally
define to be an ordered sequence of \textbf{note}s. Each note possesses a pitch
(i.e.\ frequency) and a time interval (i.e.\ start and stop times) and
represents an instruction for the performer to articulate the given pitch at
the start time and hold the note until the stop time. Multiple notes may
overlap in time, in which case all notes are to be played simultaneously.

\subsection{Pitches and tuning systems}

The \textbf{pitch} of a note refers to the frequency of sound used in its
reproduction and the difference between two pitches is called a \textbf{pitch
interval}. A fundamental pitch interval in Western music is the
\textbf{octave}, which is defined to be the interval between a frequency and
its double.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/pitch.pdf}
    \caption{Just like this text, it is the differences between pitches
        rather than the absolute pitch itself that is perceptually
    meaningful.}
    \label{fig:pitch}
\end{figure}

While in theory an uncoutable number of pitches are available because
frequencies are real-valued and hence there are uncountably many, tuning
systems are oftentimes employed to restrict the set of available pitches in
order to simplify composition, analysis, and reproduction. A \textbf{tuning
system} restricts the available pitch intervals, limiting the possible pitches
to a discrete set.

Western music commonly uses a tuning system called \textbf{12-TET} (12 tone
equal temperament) tuning, which divides an octave into 12 \textbf{pitch
classes} ($C$, $C\#/D\flat$, $D$, $D\#/E\flat$, $E$, $F$, $F\#/G\flat$, $G$,
$G\#/A\flat$, $A$, $A\#/B\flat$, $B$) all equally spaced on a logarithmic
scale. Each pitch class represents an equivalence class of frequencies which
all differ by an integral number of octaves (i.e. are doublings/halvings of
each other). This means that unambiguously specifying a pitch requires
specifying both a pitch class and an octave. Common practice numbers octaves
with respect to a standard sized keyboard, where the key corresponding to the
\textbf{middle $C$} note is referred to as $C_5$ because it is $5$ octaves
above the lowest $C$ key. The interval between two adjacent pitch classesis
called a \textbf{half step} or \textbf{half tone} and two half steps are called
a \textbf{whole step/tone}.

In our work, we restrict attention to 12-TET tuning because it is the most
common tuning system employed by Western composers over the last \todo{cite}

Note that while 12-TET restricts the available pitches to a discrete set, it
lacks specification of a reference pitch frequency and hence does not yield a
correspondance between pitches and frequencies. This degree of freedom gives
rise to \textbf{transposition invariance}: a score of music can be offset by a
constant frequency without altering characteristics arising from the pitches
present in the music. In modern practice, the general tuning standard of $A440$
defines a reference frequency of 440 Hz for the $A$ pitch class directly above
middle C.

\subsection{Tonal music}

Tonal music is characterized by the prevalence of one pitch class (the
\textbf{tonic}) around which the melody and harmony are built.

A basic concept within tonal music is the \textbf{scale} which defines a subset
of pitch classes that are ``in key'' with respect to the tonic. Two fundamental
scales are the major (with step pattern
whole-whole-half-whole-whole-whole-half) and minor scales
(whole-half-whole-whole-half-whole-whole). The choice of tonic and scale is
referred to as the \textbf{key}and a change in key during a piece is called a
\textbf{modulation}Many musical phenomena such as stability, expectation, and
resolution can be attributed to tonal characteristics.

The \textbf{tempo} of a piece refers to its speed or pace and is measured by
beats per minute. In 4/4 time signature, a \textbf{quarter note} or
\textbf{crotchet} denotes the time interval between two beats. In addition to
pitch quantization, durations are also commonly quantized to subdivisions and
multiples of a crotchet.

\subsection{Notation}

We consider note duration, time, and velocity. We neglect changes in timing
(e.g. ritardandos), dynamics (e.g. crescendos), and stylistic notations (e.g.
accents, staccatos, legatos).


\emph{Piano roll} music transcriptions (\autoref{fig:piano-roll}) are quantized
both in time ($t \in T$) and note frequencies ($n \in N$). frequencies
quantized to a piano roll. \todo{Motivate quantization with Western music}.

We can represent a piano roll transcription as a high-dimensional vecctor
$X_{t,n} \in \RR^{|T| \times |N|}$ where $X_{t,n}$ denotes the note
velocities for note $n$ at time $t$.

\section{Automatic composition}

Algorithmic composition is the application of a well-defined algorithmic
procedure to compose music.

An interesting question regarding creativity: if an algorithm faithfully reproduces
an artist's creative process, what is the difference between music produced by the artist
and music produced by the algorithm?

\section{Neural sequence probability modeling}

This section builds the notation and terminology we use to discuss the neural network
models used for generatively modelling music scores.

\subsection{Neurons: the basic computation unit}

Neurons are the basic abstraction upon which we build more complex
models. A \textbf{neuron} is a parametric model of a function $f : \RR^D \to
\RR$ defined by $f(\vec{x}) \coloneqq \sigma( \langle \vec{w}, \vec{x} \rangle
)$. \autoref{fig:nn-single} illustrates the decomposition of a neuron as first
an inner product with \textbf{weights} $\vec{w}$ followed by a non-linear
\textbf{activation function} $\sigma : \RR \to [L, U]$.

\begin{figure}[htpb]
    \centering
    \input{Figures/nn-single.pdf_tex}
    \caption{A single neuron first computes an activation $z$ and then passes it through
    an activation function $\sigma(\cdot)$}
    \label{fig:nn-single}
\end{figure}

The purpose of $\sigma$ is to squash the \textbf{activation} $z \coloneqq
\langle \vec{w}, \vec{x} \rangle$ such that the \textbf{output} $y = \sigma(z)$
lies in some bounded domain $[L, U] \subset \RR$. In addition, the
non-linearity introduced by $\sigma$ gives neural networks their
representational power. If activation functions are removed, then a neural
network can only model affine transformations.

\subsection{Composing neurons together in neural networks}

Concatenating multiple neurons into a \textbf{layer} enables multivariate functions $f :
\RR^{D_\text{in}} \to \RR^{D_\text{out}}$ to be represented, leading to feedforward
neural networks. Although a single hidden layer is theoretically sufficient for
a universal function approximator\cite{Cybenko1993}, \textbf{deep learning}
models which stack multiple layers have shown strong experimental results
for parameterizing complex unknown probability distributions \cite{Bengio2011}.

\begin{figure}[htpb]
    \centering
    \input{Figures/nn-ffw.pdf_tex}
    \caption{Graph depiction of a feedforward neural network with $2$ hidden layers}
    \label{fig:nn-ffw}
\end{figure}

When discussing neural networks with $L \geq 1$ layers, we will use
$\vec{x}^{(i)}$, $\matr{W}^{(i)}$, $\vec{z}^{(i)}$, and $\vec{y}^{(i)}$ to
refer to the inputs, weights, activations, and outputs of the $i$th layer. The
activation function $\sigma$ is understood to act elementwise when applied to a
vector. For adjacent layers $i$, $i+1$, we have $\vec{x}^{(i+1)} =
\vec{y}^{(i)}$. $\vec{x}^{(0)}$ and $\vec{y}^{(L)}$ are the inputs and outputs
respectively of the entire network.

\subsubsection{Processing variable length sequences with recursive neural networks}

\begin{figure}[htpb]
    \centering
    \input{Figures/nn-rnn.pdf_tex}
    \caption{Graph depiction of a recurrent neural network; note the recurrent connections
    within the hidden layer}
    \label{fig:nn-rnn}
\end{figure}

Recurrent neural networks (RNNs) generalize feedforward networks by introducing
time-delayed connections between hidden layers (\autoref{fig:nn-rnn}) in order
to model temporal memory. The recurrent edges enable the previous hidden state
$\vec{h}_{t-1}$ to be passed forwards to compute the current hidden state
$\vec{h}_t$. This ability to incorporate prior inputs into the network state
enables RNNs to excel at sequence processing tasks, where the input sequence
$X_n = (\vec{x}_t)_{1 \leq t \leq T_n}$, $1 \leq n \leq N$, may have variable
length $T_n$.

The time-delayed recurrent edges can be equivalently viewed as inputs to the
hidden nodes which are carried forward from the previous time step
(\autoref{fig:rnn-elman}).

\begin{figure}[htpb]
    \centering
    \input{Figures/nn-rnn-elman.pdf_tex}
    \caption{Equivalent formulation of a RNN treating the time-delayed hidden state
    as additional inputs to a feedforward network}
    \label{fig:rnn-elman}
\end{figure}

This recursive structure can be unrolled
(\autoref{fig:rnn-multi-unrolled}) to yield a directed acyclic
\textbf{computation graph} containing nodes for the
\textbf{hidden state trajectory} $H_n = (\vec{h}_t)_{1 \leq t \leq T_n}$ and
\textbf{outputs} $Y_n = (\vec{y}_t)_{1 \leq t \leq T_n}$.

\begin{figure}[htpb]
    \centering
    \input{Figures/rnn-single-unrolled.pdf_tex}
    \caption{Signal flow diagram representation of a single-layer RNN and its corresponding
    expansion into a computation graph}
    \label{fig:rnn-single-unrolled}
\end{figure}

Just like feedforward networks, RNNs can be stacked by using the outputs of one
RNN as the inputs to another, forming \textbf{deep neural sequence models}. We
will describe multilayer RNNs as a dynamical system and use
$\matr{W}^{(l)}_{ij}$ to indicate the weight matrix relating variable $i$ to
variable $j$ at layer $l$. For example, \autoref{fig:rnn-multi-unrolled} is described
by the system of equations

\begin{eqnarray}
    \vec{h}^{(1)}_t &=& \matr{W}^{(1)}_{hx} \vec{x}^{(1)}_t + \matr{W}^{(1)}_{hh} \vec{h}^{(1)}_{t-1} \\
    \vec{y}^{(1)}_t &=& \matr{W}^{(1)}_{yh} \vec{h}^{(1)}_t \\
    \vec{x}^{(2)}_t &=& \vec{y}^{(1)}_t \\
    \vec{h}^{(2)}_t &=& \matr{W}^{(2)}_{hx} \vec{x}^{(2)}_t + \matr{W}^{(2)}_{hh} \vec{h}^{(1)}_{t-1} \\
    \vec{y}^{(2)}_t &=& \matr{W}^{(2)}_{yh} \vec{h}^{(2)}_t \\
\end{eqnarray}

\begin{figure}[htpb]
    \centering
    \input{Figures/rnn-multi-unrolled.pdf_tex}
    \caption{Unrolled computation graph for a 2-layer RNN}
    \label{fig:rnn-multi-unrolled}
\end{figure}

\subsubsection{Sequential processing and sampling}

We will train our RNNs to predict a distribution for the next character
$\vec{x}_{t+1}$ after the RNN has processed the sequence $\vec{x}_{1:t}$,
yielding a model which factorizes the sequence probability
\begin{equation}
    P(\vec{x}_{1:T}) = \prod_{t=1}^T P(\vec{x}_t | \vec{x}_{1:t-1} )
\end{equation}
Modeling this probability distribution over sequences is analogous to the
\textbf{language modeling} from speech recognition.

Note that the factorization of conditional distributions \emph{assumes a
sequential ordering in $t$}. This property is desirable as it enables sampling
from the model to generate new transcriptions by sampling from $P(\vec{x}_t |
\vec{x}_{1:t-1})$ at each timestep $t$ and using the sampled value as the next
input.

\todo{compare to bidirectional}.

\todo{for more details, see XYZ}.

\subsection{Parameter estimation of recursive neural networks}

\subsubsection{Modeling probabilities using the Boltzmann distribution and cross-entropy error criterion}

The parameters to the model are estimated in order to minimize the error
between the network outputs and provided labels. For language modeling, the
outputs $\vec{y}_t$ should parameterize a distribution for the next character
$P(\vec{x}_{t+1} | \vec{x}_{1:t})$. Suppose each input has finite support (i.e.
$\vec{x}_t \in [1,2,\cdots,K]$). If we choose the outputs to be $K$ dimensional
(i.e. $\vec{y}_t \in \RR^K$) then the \textbf{Boltzmann distribution}
(\autoref{eq:boltzmann-dist}) can be used:

\begin{equation}
    \label{eq:boltzmann-dist}
    P(\vec{x}_{t+1} = s | \vec{x}_{1:t})
    = \frac{\exp \left(-\vec{y}_{t,s}/T\right) }{ \sum_{k=1}^{K} \left(\exp -\vec{y}_{t,k}/T\right)}
\end{equation}

$T \in \RR^+$ is a \textbf{temperature} parameter \todo{relate to sampling}.

The labels provided are the actual next characters $\vec{x}_{t+1}$. Viewing
such labels as discrete probability distributions will all mass on a single atom,
one measure of difference between predictions and \todo{justify cross-entropy}

\subsubsection{Efficient graident computations through back-propogation}

Feed-forward neural networks are trained using back-propogation, an efficient
algorithm which consists of a forward pass to compute activations followed by
back-propogation of partial derivatives expanded according to the chain
rule\todo{cite backprop}. At the heart of back-propogation is the
\textbf{computation graph} of a a model: a directed acyclic graph where each
node represents a differentiable function that can compute its outputs and
Jacobian given inputs and activations\todo{cite theano}. By representing only
the dependencies between intermediate values, the sparsity imposed by the
computation graph enable back-propogation to ignore irrelevant
cross-derivatives and efficiently compute global gradients from local
computations.


Training of recursive neural networks is typically performed using
backpropogation through time (BPTT)\cite{at}, a technique computationally
equivalent to feedforward training of the unrolled computation graph. This is
easily seen: unrolling of a RNN yields a feed-forward structure where the
standard back-propogation algorithm applies.




\subsubsection{Vanishing gradients}

In practice, it is observed that the hidden state does not capture long range
dependencies well and tend to suffer from vanishing/exploding gradient during
training. LSTMs are an improved RNN architecture which solve both of these
problems by introducing gates on the inputs, hidden state, and outputs. GRUs are
a variation of LSTMs which ties the weights to the input and forget gates.

Vanishing/exploding gradients \cite{Bengio1994} are problems experienced by
RNNs arising from recursive applications of non-linear activations and linear dynamics
to the hidden state. To illustrate the vanishing gradient problem, consider
a simple recurrent network

\begin{eqnarray}
    h_t &=& \theta \phi(h_{t-1}) + \theta_x x_t \label{eq:ht-from-ht-1}\\
    y_t &=& \theta_y \phi(h_t)
\end{eqnarray}

After unrolling, the gradient of the error is given as a sum of the gradients
at each individual timestep

\begin{equation}
    \frac{\pd E}{\pd \theta} = \sum_{t=1}^S \frac{\pd E_t}{\pd \theta}
\end{equation}

Since the current hidden state $h_t$ depends on all prior hidden states $h_k$ ($k < t$),
the chain rule yields

\begin{equation}
    \frac{\pd E_t}{\pd \theta} = \sum_{k=1}^t \frac{\pd E_t}{\pd y_t} \frac{\pd y_t}{\pd h_t} \frac{\pd h_t}{\pd h_k} \frac{\pd h_k}{\pd \theta}
\end{equation}

Noting that time dynamics occur only through $h_t$

\begin{equation}
    \label{eq:prod-hi}
    \frac{\pd h_t}{\pd h_k} = \prod_{i=k+1}^t \frac{\pd h_i}{\pd h_{i-1}}
\end{equation}

The factors in the product can be obtained by differentiating \autoref{eq:ht-from-ht-1}

\begin{equation}
    \frac{\pd h_i}{\pd h_{i-1}} = \theta^\tp \diag\left[ \phi'(h_{i-1}) \right]
\end{equation}

Let $\|\cdot\|$ be any submultiplicative matrix norm. For illustration purposes, we will use the
\textbf{operator norm} defined as
\begin{equation}
    \| A \| = \sup_{x \in \RR^n; x \neq 0} \frac{|A x|}{|x|}
\end{equation}
where $|\cdot|$ is the standard Euclidian norm. However, any other submultiplicative norm
(e.g. Frobenius, spectral, nuclear, Shatten $p$-norms) is equally valid.

From submultiplicativity, we have

\begin{equation}
    \left\| \frac{\pd h_i}{\pd h_{i-1}} \right\|
    \leq \|\theta^\tp\| \| \diag\left[ \phi'(h_{i-1}) \right] \|
    \leq \gamma_{\theta} \gamma_\phi
\end{equation}

where $\gamma_\theta \coloneqq \|\theta\|$ is the norm of the hidden dynamics matrix $\theta$
and
\begin{align}
    \gamma_\phi
    &\coloneqq \sup_{h \in \RR^n} \| \diag \left[ \phi'(h) \right] \|  &\\
    &= \sup_{h \in \RR^n} \max_i \phi'(h)_i &\mbox{Operator norm of diag} \\
    &= \sup_{t \in \RR} \phi'(t) &\mbox{$\phi$ operates elementwise}
\end{align}

For common activation functions $\gamma_{\phi} \leq \infty$ (e.g. $\gamma_\phi = 1$ for $\phi = \tanh$
and $\gamma_\phi = 1/4$ for $\phi = \sigma$).

This gives a bound on \autoref{eq:prod-hi}, namely

\begin{equation}
    \left\| \frac{\pd h_t}{\pd h_k} \right\|
    = \left\| \prod_{i=k+1}^{t} \frac{\pd h_t}{\pd h_k} \right\|
    \leq (\gamma_\theta \gamma_\phi)^{t-k}
\end{equation}

When $|\gamma_\theta \gamma_\phi| < 1$, these gradients exponentially vanish to zero as
the time interval $t - k$ increases, hindering the network's ability to learn long-range
dynamics.

A similar argument \cite{Bengio1994} illustrates an analogous exponential explosion
when $\|\theta\| > 1$ (the ``exploding gradient'' problem).

The solution is to rewrite \autoref{eq:ht-from-ht-1} such that
\autoref{eq:prod-hi} does not vanish/explode for large $t - k$.
One possibility would be

\begin{equation}
    h_t = h_{t-1} + \theta_x x_t
\end{equation}

However, this solution is unsatisfactory as all hidden state dynamics have been
removed.

\subsubsection{Long Short Term Memory}

LSTMs are a RNN architecture which mitigates vanishing/exploding gradients by constraining
the hidden state dynamics. It does so by constraining the nonlinearity $\phi = \matr{I}$ to be the
identity and introducing three gates:
\begin{itemize}
    \item \textbf{Input gate}: scales input $x_t$ elementwise by $i_t \in [0,1]$, affects how $h_t$ is written to
    \item \textbf{Output gate}: scales output $y_t$ elementwise by $o_t \in [0,1]$, affects how $h_t$ is read from
    \item \textbf{Forget gate}: scales previous cell value $h_{t-1}$ by $f_t \in [0,1]$, acts as a reset mechanism
\end{itemize}

\begin{figure}[htpb]
    \centering
    \input{Figures/lstm-unit.pdf_tex}
    \caption{Single LSTM unit}
    \label{fig:lstm-unit}
\end{figure}

The equations for an LSTM are

\begin{eqnarray}
    i_t &=& \sigma(W_{xi} x_t + W_{yi} y_{t-1} + W_{ci} c_{t-1} + b_i) \\
    o_t &=& \sigma(W_{xo} x_t + W_{yo} y_{t-1} + W_{co} c_{t-1} + b_o) \\
    f_t &=& \sigma(W_{xf} x_t + W_{yf} y_{t-1} + W_{cf} c_{t-1} + b_f) \\
    h_t &=& f_t \odot h_{t-1} + i_t \odot \tanh(x_t W_{xc} + h_{t-1} W_{hc} + b_c) \label{eq:lstm-dynamics} \\
    y_t &=& o_t \odot \tanh(c_t)
\end{eqnarray}
where $\odot$ denotes elementwise multiplication of vectors.

Note that \autoref{eq:lstm-dynamics} has $\phi$ set to the identity function
hence $\gamma_\phi = 1$. This removal of the non-linearity in the hidden state
recursion is known as the \textbf{constant error carousel} \todo{cite}, which
allows the network to propogate errors without modification by $\varphi'(h_k)$.

The addition of the gates enables the model to learn when to read/write from
memory and when to reset, enabling longer-range dependencies to be learned when
an input is written to memory (i.e. $i_t$ high) and protected for a long
duration (i.e. $f_t \approx 1$ and $i_t \approx 0$).

Some authors define LSTMs such that $h_t$ is not used to compute gate activations,
referring to the case where $h_t$ is connected as ``peephole connections.'' In our
work, we use ``LSTM'' to refer to LSTMs with peephole connections.

LSTMs protect from vanishing gradient, but not exploding gradient. Research has shown
that gradient clipping is essential to permit successful applications \cite{Pascanu2012}.
